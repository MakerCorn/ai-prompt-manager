"""
Comprehensive unit tests for LangWatch Optimizer module
Testing prompt optimization functionality across multiple services
"""

import os
from datetime import datetime
from unittest.mock import patch

import pytest

from langwatch_optimizer import OptimizationResult, PromptOptimizer


class TestPromptOptimizer:
    """Test suite for PromptOptimizer functionality"""

    @pytest.fixture
    def optimizer_builtin(self):
        """Create PromptOptimizer with builtin service"""
        with patch.dict(os.environ, {"PROMPT_OPTIMIZER": "builtin"}, clear=True):
            return PromptOptimizer()

    @pytest.fixture
    def optimizer_langwatch(self):
        """Create PromptOptimizer with LangWatch service"""
        with patch.dict(
            os.environ,
            {
                "PROMPT_OPTIMIZER": "langwatch",
                "LANGWATCH_API_KEY": "test_api_key",
                "LANGWATCH_PROJECT_ID": "test_project",
            },
            clear=True,
        ):
            return PromptOptimizer()

    @pytest.fixture
    def optimizer_promptperfect(self):
        """Create PromptOptimizer with PromptPerfect service"""
        with patch.dict(
            os.environ,
            {
                "PROMPT_OPTIMIZER": "promptperfect",
                "PROMPTPERFECT_API_KEY": "test_api_key",
            },
            clear=True,
        ):
            return PromptOptimizer()

    @pytest.fixture
    def optimizer_langsmith(self):
        """Create PromptOptimizer with LangSmith service"""
        with patch.dict(
            os.environ,
            {
                "PROMPT_OPTIMIZER": "langsmith",
                "LANGSMITH_API_KEY": "test_api_key",
                "LANGSMITH_PROJECT": "test_project",
            },
            clear=True,
        ):
            return PromptOptimizer()

    @pytest.fixture
    def optimizer_helicone(self):
        """Create PromptOptimizer with Helicone service"""
        with patch.dict(
            os.environ,
            {
                "PROMPT_OPTIMIZER": "helicone",
                "HELICONE_API_KEY": "test_api_key",
                "HELICONE_APP_NAME": "test_app",
            },
            clear=True,
        ):
            return PromptOptimizer()

    @pytest.fixture
    def sample_prompt(self):
        """Sample prompt for testing"""
        return "Write a story about a cat"

    @pytest.fixture
    def complex_prompt(self):
        """Complex prompt for testing"""
        return """
        Please help me with this task. I need you to write something about cats.
        Make it good. It should be interesting and fun to read.
        I want it to be creative but also informative.
        """

    @pytest.fixture
    def vague_prompt(self):
        """Vague prompt that needs optimization"""
        return "Do something with data"

    def test_optimization_result_dataclass(self):
        """Test OptimizationResult dataclass creation"""
        result = OptimizationResult(
            optimized_prompt="Optimized prompt",
            original_prompt="Original prompt",
            optimization_score=85.5,
            suggestions=["Make it clearer", "Add specificity"],
            reasoning="Improved clarity and structure",
            timestamp=datetime.now(),
            success=True,
            error_message=None,
        )

        assert result.optimized_prompt == "Optimized prompt"
        assert result.original_prompt == "Original prompt"
        assert result.optimization_score == 85.5
        assert len(result.suggestions) == 2
        assert result.success is True
        assert result.error_message is None

    def test_optimizer_initialization_default(self):
        """Test PromptOptimizer initialization with default settings"""
        with patch.dict(os.environ, {}, clear=True):
            optimizer = PromptOptimizer()
            assert optimizer.service == "builtin"
            assert optimizer.is_available()

    def test_optimizer_initialization_builtin(self, optimizer_builtin):
        """Test PromptOptimizer initialization with builtin service"""
        assert optimizer_builtin.service == "builtin"
        assert optimizer_builtin.is_available()

    def test_optimizer_initialization_langwatch_with_key(self, optimizer_langwatch):
        """Test PromptOptimizer initialization with LangWatch service and API key"""
        assert optimizer_langwatch.service == "langwatch"
        assert optimizer_langwatch.config["langwatch"]["api_key"] == "test_api_key"
        assert optimizer_langwatch.config["langwatch"]["project_id"] == "test_project"
        assert optimizer_langwatch.is_available()

    def test_optimizer_initialization_langwatch_without_key(self):
        """Test PromptOptimizer initialization with LangWatch service but no API key"""
        with patch.dict(os.environ, {"PROMPT_OPTIMIZER": "langwatch"}, clear=True):
            optimizer = PromptOptimizer()
            # Should fallback to builtin
            assert optimizer.service == "builtin"

    def test_optimizer_initialization_promptperfect_with_key(
        self, optimizer_promptperfect
    ):
        """Test PromptOptimizer initialization with PromptPerfect service and API key"""
        assert optimizer_promptperfect.service == "promptperfect"
        assert (
            optimizer_promptperfect.config["promptperfect"]["api_key"] == "test_api_key"
        )
        assert optimizer_promptperfect.is_available()

    def test_optimizer_initialization_langsmith_with_key(self, optimizer_langsmith):
        """Test PromptOptimizer initialization with LangSmith service and API key"""
        assert optimizer_langsmith.service == "langsmith"
        assert optimizer_langsmith.config["langsmith"]["api_key"] == "test_api_key"
        assert optimizer_langsmith.config["langsmith"]["project"] == "test_project"
        assert optimizer_langsmith.is_available()

    def test_optimizer_initialization_helicone_with_key(self, optimizer_helicone):
        """Test PromptOptimizer initialization with Helicone service and API key"""
        assert optimizer_helicone.service == "helicone"
        assert optimizer_helicone.config["helicone"]["api_key"] == "test_api_key"
        assert optimizer_helicone.config["helicone"]["app_name"] == "test_app"
        assert optimizer_helicone.is_available()

    def test_get_status_builtin(self, optimizer_builtin):
        """Test get_status for builtin service"""
        status = optimizer_builtin.get_status()

        assert isinstance(status, dict)
        assert status["service"] == "builtin"
        assert status["available"] is True
        assert status["api_key_set"] is False
        assert "builtin" in status["services_available"]

    def test_get_status_langwatch(self, optimizer_langwatch):
        """Test get_status for LangWatch service"""
        status = optimizer_langwatch.get_status()

        assert status["service"] == "langwatch"
        assert status["available"] is True
        assert status["api_key_set"] is True
        assert "langwatch" in status["services_available"]
        assert status["config"]["api_key"] == "***"  # Should be masked

    def test_get_status_service_fallback(self):
        """Test get_status when service falls back to builtin"""
        with patch.dict(
            os.environ, {"PROMPT_OPTIMIZER": "invalid_service"}, clear=True
        ):
            optimizer = PromptOptimizer()
            status = optimizer.get_status()

            assert status["service"] == "builtin"
            assert status["available"] is True

    def test_optimize_prompt_basic_builtin(self, optimizer_builtin, sample_prompt):
        """Test basic prompt optimization with builtin service"""
        result = optimizer_builtin.optimize_prompt(sample_prompt)

        assert isinstance(result, OptimizationResult)
        assert result.success is True
        assert result.error_message is None
        assert result.original_prompt == sample_prompt
        assert (
            result.optimized_prompt is not None
        )  # May or may not be different from original
        assert 0 <= result.optimization_score <= 100
        assert isinstance(result.suggestions, list)
        assert len(result.suggestions) > 0
        assert isinstance(result.reasoning, str)
        assert len(result.reasoning) > 0
        assert isinstance(result.timestamp, datetime)

    def test_optimize_prompt_with_context(self, optimizer_builtin, sample_prompt):
        """Test prompt optimization with context"""
        context = "This is for a creative writing exercise"
        result = optimizer_builtin.optimize_prompt(sample_prompt, context=context)

        assert result.success is True
        assert (
            result.optimized_prompt is not None
        )  # May or may not be different from original
        # Context is passed to optimization (though may not appear directly in reasoning)
        assert result.reasoning is not None
        assert len(result.reasoning) > 0

    def test_optimize_prompt_with_target_model(self, optimizer_builtin, sample_prompt):
        """Test prompt optimization with specific target model"""
        result = optimizer_builtin.optimize_prompt(
            sample_prompt, target_model="claude-3"
        )

        assert result.success is True
        assert (
            result.optimized_prompt is not None
        )  # May or may not be different from original

    def test_optimize_prompt_with_custom_goals(self, optimizer_builtin, sample_prompt):
        """Test prompt optimization with custom goals"""
        goals = ["clarity", "specificity"]
        result = optimizer_builtin.optimize_prompt(
            sample_prompt, optimization_goals=goals
        )

        assert result.success is True
        assert (
            result.optimized_prompt is not None
        )  # May or may not be different from original
        # Should have meaningful reasoning that includes general optimization concepts
        assert (
            "clarity" in result.reasoning.lower()
            or "optimization" in result.reasoning.lower()
        )

    def test_optimize_prompt_empty_input(self, optimizer_builtin):
        """Test prompt optimization with empty input"""
        result = optimizer_builtin.optimize_prompt("")

        assert result.success is False
        assert result.error_message is not None
        assert (
            "empty" in result.error_message.lower()
            or "invalid" in result.error_message.lower()
        )

    def test_optimize_prompt_whitespace_only(self, optimizer_builtin):
        """Test prompt optimization with whitespace-only input"""
        result = optimizer_builtin.optimize_prompt("   \n\t   ")

        assert result.success is False
        assert result.error_message is not None

    def test_optimize_prompt_very_long(self, optimizer_builtin):
        """Test prompt optimization with very long input"""
        long_prompt = "Write a story about a cat. " * 1000  # Very long prompt
        result = optimizer_builtin.optimize_prompt(long_prompt)

        assert result.success is True
        # Should handle long prompts gracefully (may or may not modify them)
        assert result.optimized_prompt is not None
        assert result.suggestions is not None

    def test_optimize_prompt_vague_input(self, optimizer_builtin, vague_prompt):
        """Test optimization of vague prompt"""
        result = optimizer_builtin.optimize_prompt(vague_prompt)

        assert result.success is True
        # The builtin optimizer may or may not change the prompt depending on content
        # but should always provide suggestions and reasoning
        assert result.suggestions is not None
        assert len(result.suggestions) > 0
        assert result.optimization_score >= 0  # Should have some score

    def test_optimize_prompt_complex_input(self, optimizer_builtin, complex_prompt):
        """Test optimization of complex prompt"""
        result = optimizer_builtin.optimize_prompt(complex_prompt)

        assert result.success is True
        # The complex prompt may or may not be changed, but should be processed
        assert result.optimized_prompt is not None
        assert result.suggestions is not None

    def test_optimization_different_services(self, sample_prompt):
        """Test that different services produce different optimizations"""
        results = {}

        # Test builtin
        with patch.dict(os.environ, {"PROMPT_OPTIMIZER": "builtin"}, clear=True):
            optimizer = PromptOptimizer()
            results["builtin"] = optimizer.optimize_prompt(sample_prompt)

        # Test langwatch (mocked)
        with patch.dict(
            os.environ,
            {"PROMPT_OPTIMIZER": "langwatch", "LANGWATCH_API_KEY": "test_key"},
            clear=True,
        ):
            optimizer = PromptOptimizer()
            results["langwatch"] = optimizer.optimize_prompt(sample_prompt)

        # Test promptperfect (mocked)
        with patch.dict(
            os.environ,
            {"PROMPT_OPTIMIZER": "promptperfect", "PROMPTPERFECT_API_KEY": "test_key"},
            clear=True,
        ):
            optimizer = PromptOptimizer()
            results["promptperfect"] = optimizer.optimize_prompt(sample_prompt)

        # All should succeed
        for service, result in results.items():
            assert result.success is True, f"Service {service} failed"

        # Different services might produce different results
        optimized_prompts = [result.optimized_prompt for result in results.values()]
        # At least some variation expected (though mocked services might be similar)
        assert len(optimized_prompts) > 0, "Should have optimized prompts"

    def test_builtin_optimization_rules(self, optimizer_builtin):
        """Test specific builtin optimization rules"""
        test_cases = [
            ("write something", "Write a detailed"),  # Should add specificity
            ("make it good", "Create a high-quality"),  # Should replace vague terms
            ("do this task", "Complete this specific task"),  # Should add clarity
        ]

        for input_prompt, expected_improvement in test_cases:
            result = optimizer_builtin.optimize_prompt(input_prompt)
            assert result.success is True
            # Check that some improvement was made
            assert len(result.optimized_prompt) > len(input_prompt)

    def test_optimization_score_calculation(self, optimizer_builtin):
        """Test optimization score calculation"""
        # Test with prompts of different quality
        prompts = [
            "Write",  # Very vague
            "Write a story",  # Somewhat vague
            "Write a detailed 500-word story about a mysterious cat in a Victorian mansion",  # Already detailed
        ]

        scores = []
        for prompt in prompts:
            result = optimizer_builtin.optimize_prompt(prompt)
            scores.append(result.optimization_score)

        # All should have valid scores
        for score in scores:
            assert 0 <= score <= 100

    def test_suggestions_generation(self, optimizer_builtin, sample_prompt):
        """Test that optimization suggestions are meaningful"""
        result = optimizer_builtin.optimize_prompt(sample_prompt)

        assert len(result.suggestions) > 0
        for suggestion in result.suggestions:
            assert isinstance(suggestion, str)
            assert len(suggestion) > 0
            # Suggestions should be actionable advice or service-specific feedback
            assert any(
                word in suggestion.lower()
                for word in [
                    "specific",
                    "clear",
                    "detail",
                    "structure",
                    "improve",
                    "add",
                    "consider",
                    "applied",
                    "optimization",
                    "builtin",
                    "best",
                    "practices",
                ]
            )

    def test_reasoning_generation(self, optimizer_builtin, sample_prompt):
        """Test that optimization reasoning is meaningful"""
        result = optimizer_builtin.optimize_prompt(sample_prompt)

        assert isinstance(result.reasoning, str)
        assert len(result.reasoning) > 0
        # Reasoning should explain what was done
        assert any(
            word in result.reasoning.lower()
            for word in [
                "improve",
                "added",
                "enhanced",
                "structured",
                "clarified",
                "optimization",
                "applied",
            ]
        )

    def test_optimization_goals_influence(self, optimizer_builtin, sample_prompt):
        """Test that optimization goals influence the results"""
        # Test with different goals
        clarity_result = optimizer_builtin.optimize_prompt(
            sample_prompt, optimization_goals=["clarity"]
        )
        specificity_result = optimizer_builtin.optimize_prompt(
            sample_prompt, optimization_goals=["specificity"]
        )
        conciseness_result = optimizer_builtin.optimize_prompt(
            sample_prompt, optimization_goals=["conciseness"]
        )

        # All should succeed
        assert clarity_result.success is True
        assert specificity_result.success is True
        assert conciseness_result.success is True

        # All results should have meaningful reasoning
        assert clarity_result.reasoning is not None
        assert specificity_result.reasoning is not None
        assert conciseness_result.reasoning is not None

    def test_error_handling_service_failure(self, optimizer_builtin):
        """Test error handling when optimization service fails"""
        # Mock internal method to raise exception
        with patch.object(
            optimizer_builtin,
            "_apply_builtin_rules",
            side_effect=Exception("Service error"),
        ):
            result = optimizer_builtin.optimize_prompt("Test prompt")

            assert result.success is False
            assert result.error_message is not None
            assert "error" in result.error_message.lower()

    def test_edge_cases_special_characters(self, optimizer_builtin):
        """Test optimization with special characters"""
        special_prompt = (
            "Write about: cats & dogs, their #1 traits! (be creative) @user"
        )
        result = optimizer_builtin.optimize_prompt(special_prompt)

        assert result.success is True
        # Should handle special characters gracefully (may or may not change the prompt)
        assert result.optimized_prompt is not None
        assert result.suggestions is not None

    def test_edge_cases_unicode(self, optimizer_builtin):
        """Test optimization with Unicode characters"""
        unicode_prompt = "Write about cats in franÃ§ais, ä¸­æ–‡, and espaÃ±ol ðŸ±"
        result = optimizer_builtin.optimize_prompt(unicode_prompt)

        assert result.success is True
        # Should handle Unicode gracefully

    def test_optimization_consistency(self, optimizer_builtin, sample_prompt):
        """Test that optimization is consistent across multiple calls"""
        results = []
        for _ in range(3):
            result = optimizer_builtin.optimize_prompt(sample_prompt)
            results.append(result)

        # All should succeed
        for result in results:
            assert result.success is True

        # Results should be identical (deterministic)
        first_result = results[0]
        for result in results[1:]:
            assert result.optimized_prompt == first_result.optimized_prompt
            assert result.optimization_score == first_result.optimization_score

    def test_service_availability_checks(self):
        """Test service availability checking"""
        # Test with missing API keys
        test_configs = [
            {"PROMPT_OPTIMIZER": "langwatch"},  # No API key
            {"PROMPT_OPTIMIZER": "promptperfect"},  # No API key
            {"PROMPT_OPTIMIZER": "langsmith"},  # No API key
            {"PROMPT_OPTIMIZER": "helicone"},  # No API key
        ]

        for config in test_configs:
            with patch.dict(os.environ, config, clear=True):
                optimizer = PromptOptimizer()
                # Should fallback to builtin
                assert optimizer.service == "builtin"
                assert optimizer.is_available()

    def test_default_values_handling(self, optimizer_builtin):
        """Test handling of default values"""
        result = optimizer_builtin.optimize_prompt(
            "Test prompt"
            # All other parameters use defaults
        )

        assert result.success is True
        # Should use default target_model and optimization_goals

    def test_none_values_handling(self, optimizer_builtin):
        """Test handling of None values"""
        result = optimizer_builtin.optimize_prompt(
            "Test prompt",
            context=None,
            target_model=None,
            optimization_goals=None,
        )

        assert result.success is True
        # Should handle None values gracefully

    def test_create_optimization_context(self, optimizer_builtin):
        """Test optimization context creation"""
        context = optimizer_builtin._create_optimization_context(
            "Test prompt", "Test context", "gpt-4", ["clarity", "specificity"]
        )

        assert isinstance(context, str)
        assert "Test prompt" in context
        assert "Test context" in context
        assert "gpt-4" in context
        assert "clarity" in context
        assert "specificity" in context

    def test_calculate_optimization_score_edge_cases(self, optimizer_builtin):
        """Test optimization score calculation edge cases"""
        # Same prompt - implementation uses base score of 50
        score1 = optimizer_builtin._calculate_optimization_score(
            "Test prompt", "Test prompt"
        )
        assert score1 == 50.0  # Base score in implementation

        # Much longer optimized prompt
        score2 = optimizer_builtin._calculate_optimization_score(
            "Test",
            "This is a detailed, specific, and well-structured test prompt that provides clear instructions",
        )
        assert score2 > 50

        # Shorter optimized prompt (conciseness)
        score3 = optimizer_builtin._calculate_optimization_score(
            "This is a very long and verbose prompt that could be made much shorter",
            "Brief, clear prompt",
        )
        assert score3 >= 50  # Base score

    def test_all_optimization_rules_applied(self, optimizer_builtin):
        """Test that optimization rules are being applied"""
        # Create a prompt with "help me" that should trigger rules
        poor_prompt = (
            "help me do something with the thing and make it good please write stuff"
        )

        result = optimizer_builtin.optimize_prompt(poor_prompt)

        assert result.success is True
        # This prompt contains "help me" which should trigger a rule
        assert result.optimized_prompt != poor_prompt
        assert result.optimization_score > 50  # Should show improvement

        # Should have meaningful suggestions
        assert result.suggestions is not None
        assert len(result.suggestions) > 0

    def test_concurrent_optimization_safety(self, optimizer_builtin, sample_prompt):
        """Test thread safety of optimization operations"""
        # Simulate concurrent optimizations
        results = []
        for i in range(5):
            result = optimizer_builtin.optimize_prompt(f"{sample_prompt} {i}")
            results.append(result)

        # All should succeed
        for result in results:
            assert result.success is True

    def test_memory_efficiency_large_prompts(self, optimizer_builtin):
        """Test memory efficiency with large prompts"""
        # Create a very large prompt
        large_prompt = "Write a detailed story about cats. " * 1000

        result = optimizer_builtin.optimize_prompt(large_prompt)

        # Should handle large prompts without memory issues
        assert result.success is True
        assert isinstance(result.optimized_prompt, str)
