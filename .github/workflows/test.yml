name: Test and Validation

on:
  workflow_run:
    workflows: ["Build and Publish Package"]
    types:
      - completed
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test-python:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'pull_request' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Update lock file
        run: poetry lock

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'
        run: poetry install --no-interaction --no-root

      - name: Install project
        run: poetry install --no-interaction

      - name: Test unified launcher
        run: |
          # Test launcher help and basic functionality
          poetry run python run.py --help
          echo "✅ Launcher help command works"
          
          # Test configuration parsing
          poetry run python -c "
          import sys
          import os
          sys.path.insert(0, '.')
          from run import get_configuration, parse_arguments
          
          # Test environment variable configuration
          os.environ['MULTITENANT_MODE'] = 'false'
          os.environ['ENABLE_API'] = 'true'
          
          class MockArgs:
              single_user = False
              multi_tenant = False
              with_api = False
              host = None
              port = None
              debug = False
              share = False
          
          config = get_configuration(MockArgs())
          assert config['multitenant_mode'] == False
          assert config['enable_api'] == True
          print('✅ Configuration parsing works correctly')
          "

      - name: Run comprehensive unit tests
        run: |
          echo "🧪 Running comprehensive unit test suite..."
          
          # Run all unit tests with verbose output and coverage
          echo "📋 Running unit tests with detailed output..."
          poetry run pytest tests/unit/ -v --tb=short --durations=10
          
          # Count total tests
          UNIT_TEST_COUNT=$(poetry run pytest tests/unit/ --collect-only -q | grep -E "^<|::test" | wc -l)
          echo "📊 Total unit tests found: $UNIT_TEST_COUNT"
          
          echo "✅ Unit test suite completed successfully"

      - name: Run comprehensive integration tests
        run: |
          echo "🧪 Running comprehensive integration test suite..."
          
          # Run all integration tests systematically
          echo "📋 Integration Tests Summary:"
          
          # Test 1: LangWatch Integration
          echo "🤖 Testing LangWatch integration..."
          poetry run python tests/integration/test_langwatch_integration.py
          echo "✅ LangWatch integration test completed"
          
          # Test 2: Multi-tenant Installation
          echo "🏢 Testing multi-tenant installation..."
          poetry run python tests/integration/test_mt_install.py
          echo "✅ Multi-tenant installation test completed"
          
          # Test 3: New Architecture Integration
          echo "🏗️ Testing new architecture integration..."
          poetry run python tests/integration/test_new_architecture_integration.py
          echo "✅ New architecture integration test completed"
          
          # Test 4: API Integration (comprehensive)
          echo "🔌 Testing comprehensive API integration..."
          poetry run python tests/integration/test_api_integration.py
          echo "✅ API integration test completed"
          
          # Test 5: Standalone API
          echo "⚡ Testing standalone API..."
          poetry run python tests/integration/test_standalone_api.py
          echo "✅ Standalone API test completed"
          
          # Test 6: Azure Integration (if configured)
          echo "☁️ Testing Azure integration..."
          poetry run python tests/integration/test_azure_integration.py || echo "⚠️ Azure integration test skipped (likely due to missing credentials)"
          
          echo "✅ All integration tests completed"

      - name: Test new architecture components
        run: |
          echo "🧪 Testing new architecture components..."
          
          # Test the new prompt architecture with repository fixes
          poetry run python tests/integration/test_new_prompt_architecture.py
          
          # Test repository save functionality specifically
          poetry run python -c "
          import sys
          import tempfile
          sys.path.insert(0, 'src')
          
          from src.core.config.settings import DatabaseConfig, DatabaseType
          from src.core.base.database_manager import DatabaseManager
          from src.prompts.services.prompt_service import PromptService
          
          # Test repository save fix
          temp_db = tempfile.NamedTemporaryFile(delete=False, suffix='.db')
          temp_db.close()
          
          try:
              db_config = DatabaseConfig(db_type=DatabaseType.SQLITE, db_path=temp_db.name)
              db_manager = DatabaseManager(db_config)
              prompt_service = PromptService(db_manager)
              
              # Test the fixed repository save functionality
              result = prompt_service.create_prompt(
                  tenant_id='test-tenant',
                  user_id='test-user',
                  name='ci_test_prompt',
                  title='CI Test Prompt',
                  content='Testing repository save fix in CI'
              )
              
              if result.success and result.data and result.data.id:
                  print('✅ Repository save fix verified in CI')
              else:
                  print(f'❌ Repository save test failed: {result.error}')
                  exit(1)
          except Exception as e:
              print(f'❌ Repository test failed with exception: {e}')
              exit(1)
          finally:
              import os
              try:
                  os.unlink(temp_db.name)
              except:
                  pass
          "
          
          # Test configuration system
          poetry run python -c "
          import sys
          import os
          sys.path.insert(0, 'src')
          
          from src.core.config.settings import AppConfig
          
          # Test environment configuration loading
          os.environ['SECRET_KEY'] = 'test-secret-key'
          config = AppConfig.from_env()
          print(f'✅ Configuration loaded: {config.auth.secret_key[:10]}...')
          "
          
          echo "✅ New architecture tests completed"

      - name: Test standalone API endpoints
        run: |
          # Test the standalone API (as used in the workflow tests)
          echo "Starting standalone API server..."
          poetry run python -c "
          import uvicorn
          import logging
          from api_endpoints import get_api_app
          
          logging.basicConfig(level=logging.INFO)
          print('Starting standalone API server...', flush=True)
          app = get_api_app()
          uvicorn.run(app, host='127.0.0.1', port=7861, log_level='info')
          " > api_server.log 2>&1 &
          API_PID=$!
          echo "API server started with PID: $API_PID"
          
          # Function to check if API server is ready
          check_api_ready() {
            local max_attempts=60
            local attempt=1
            
            echo "Waiting for standalone API to be ready..."
            while [ $attempt -le $max_attempts ]; do
              echo "Attempt $attempt of $max_attempts..."
              
              # Check if process is still running
              if ! kill -0 $API_PID 2>/dev/null; then
                echo "ERROR: API server process died!"
                echo "=== API Server Log ==="
                cat api_server.log || echo "No log file found"
                return 1
              fi
              
              # Try to connect to health endpoint
              if curl -s --max-time 5 http://127.0.0.1:7861/api/health >/dev/null 2>&1; then
                echo "✅ Standalone API is ready!"
                return 0
              fi
              
              # Show progress every 10 attempts
              if [ $((attempt % 10)) -eq 0 ]; then
                echo "=== Recent API Log ==="
                tail -5 api_server.log 2>/dev/null || echo "No log available yet"
              fi
              
              sleep 1
              attempt=$((attempt + 1))
            done
            
            echo "❌ Standalone API failed to become ready within 60 seconds"
            echo "=== Full API Log ==="
            cat api_server.log || echo "No log file found"
            return 1
          }
          
          # Wait for API to be ready
          if check_api_ready; then
            echo "Proceeding with standalone API tests..."
            
            # Test health endpoint
            echo "Testing health endpoint..."
            if curl -f -v http://127.0.0.1:7861/api/health; then
              echo "✅ Health endpoint test passed"
            else
              echo "❌ Health endpoint test failed"
              kill $API_PID 2>/dev/null || true
              exit 1
            fi
            
            # Test unauthenticated endpoint (should return 403)
            echo "Testing unauthenticated endpoint..."
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:7861/api/prompts)
            echo "HTTP response code: $HTTP_CODE"
            
            if [ "$HTTP_CODE" = "403" ]; then
              echo "✅ Authentication test passed (got 403 as expected)"
            else
              echo "⚠️ Authentication test got $HTTP_CODE (may be expected in some configurations)"
            fi
            
            echo "✅ All standalone API tests passed"
          else
            echo "❌ Standalone API startup failed"
            exit 1
          fi
          
          # Clean up
          echo "Stopping API server (PID: $API_PID)..."
          kill $API_PID 2>/dev/null || true
          sleep 2
          if kill -0 $API_PID 2>/dev/null; then
            echo "Force killing API server..."
            kill -9 $API_PID 2>/dev/null || true
          fi
          echo "✅ Standalone API test cleanup completed"

      - name: Generate comprehensive test report
        run: |
          echo "📊 Comprehensive Test Report"
          echo "================================"
          
          # Count unit tests
          UNIT_TEST_COUNT=$(find tests/unit -name "test_*.py" | wc -l)
          echo "📋 Unit Test Files: $UNIT_TEST_COUNT"
          
          # Count integration tests  
          INTEGRATION_TEST_COUNT=$(find tests/integration -name "test_*.py" | wc -l)
          echo "📋 Integration Test Files: $INTEGRATION_TEST_COUNT"
          
          # Count E2E tests
          E2E_TEST_COUNT=$(find tests/e2e -name "test_*.py" | wc -l)
          echo "📋 E2E Test Files: $E2E_TEST_COUNT"
          
          # Total test files
          TOTAL_TEST_FILES=$((UNIT_TEST_COUNT + INTEGRATION_TEST_COUNT + E2E_TEST_COUNT))
          echo "📋 Total Test Files: $TOTAL_TEST_FILES"
          
          # List all test files for reference
          echo ""
          echo "📁 Unit Test Files:"
          find tests/unit -name "test_*.py" | sort | sed 's/^/  - /'
          
          echo ""
          echo "📁 Integration Test Files:"  
          find tests/integration -name "test_*.py" | sort | sed 's/^/  - /'
          
          echo ""
          echo "📁 E2E Test Files:"
          find tests/e2e -name "test_*.py" | sort | sed 's/^/  - /'
          
          echo ""
          echo "✅ All tests completed successfully!"
          echo "🎯 Test Coverage: Comprehensive unit, integration, and E2E testing"
          echo "🚀 Ready for deployment!"

  e2e-tests:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'pull_request' }}
    needs: test-python
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Poetry
        uses: snok/install-poetry@v1
        with:
          virtualenvs-create: true
          virtualenvs-in-project: true

      - name: Load cached venv
        id: cached-poetry-dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/poetry.lock') }}

      - name: Install dependencies including E2E
        run: |
          poetry install --no-interaction --with e2e

      - name: Install Playwright browsers
        run: |
          poetry run playwright install chromium --with-deps

      - name: Set up display for headless testing
        run: |
          export DISPLAY=:99
          sudo Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &

      - name: Run E2E tests
        env:
          E2E_HEADLESS: "true"
          E2E_SLOW_MO: "0"
        run: |
          echo "🎭 Running End-to-End Test Suite"
          echo "================================="
          
          # Create test results directory
          mkdir -p e2e-results
          
          # Run E2E tests with detailed output
          echo "🚀 Starting E2E test execution..."
          poetry run pytest tests/e2e/ -v \
            --tb=short \
            --durations=10 \
            --html=e2e-results/report.html \
            --self-contained-html \
            -m "e2e" \
            --maxfail=5 \
            --timeout=300
          
          echo "✅ E2E test suite completed successfully"

      - name: Generate E2E test report
        if: always()
        run: |
          echo "📊 E2E Test Report Summary"
          echo "=========================="
          
          # Count E2E test files
          E2E_TEST_COUNT=$(find tests/e2e -name "test_*.py" | wc -l)
          echo "📋 E2E Test Files: $E2E_TEST_COUNT"
          
          # List E2E test files
          echo ""
          echo "📁 E2E Test Files:"
          find tests/e2e -name "test_*.py" | sort | sed 's/^/  - /'
          
          echo ""
          echo "🎯 E2E Test Categories:"
          echo "  - Authentication Flow Tests"
          echo "  - Prompt Management Workflow Tests"
          echo "  - API Workflow Tests"
          echo "  - Deployment Scenario Tests"
          
          echo ""
          echo "✅ E2E testing completed"

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: e2e-results/
          retention-days: 7

  lint-and-format:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'pull_request' }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Poetry
        uses: snok/install-poetry@v1

      - name: Install dependencies
        run: poetry install --no-interaction

      - name: Check Python syntax
        run: |
          # Check legacy Python files
          poetry run python -m py_compile *.py
          echo "✅ Legacy Python syntax check passed"
          
          # Check new architecture Python files
          find src -name "*.py" -exec poetry run python -m py_compile {} \;
          echo "✅ New architecture Python syntax check passed"

      - name: Check imports
        run: |
          poetry run python -c "
          import prompt_manager
          import prompt_data_manager
          import auth_manager
          import api_token_manager
          import api_endpoints
          import langwatch_optimizer
          print('✅ All legacy imports successful')
          "
          
          # Test new architecture imports
          poetry run python -c "
          import sys
          import os
          sys.path.insert(0, 'src')
          
          # Test new architecture components
          from src.core.config.settings import AppConfig
          from src.core.base.database_manager import DatabaseManager
          from src.prompts.models.prompt import Prompt
          from src.prompts.repositories.prompt_repository import PromptRepository
          from src.prompts.services.prompt_service import PromptService
          from src.auth.models.user import User
          from src.auth.models.tenant import Tenant
          print('✅ All new architecture imports successful')
          "